---
title: 'Exercises 1-4 '
author: "Giovanni Zurlo"
date: "3/10/2021"
output:
  html_document: default
  pdf_document: default
---
***


## **Exercise 1**
> *Run K-means for the Olive Oil data with K = 3 and K = 9 with scaled and unscaled data. Assuming that the macro-areas are the "true" clusters for K = 3, use table to compare the macro-areas with the clustering. Do you think that these are good clustering results in terms of matching the macro-areas? Why? Does the clustering on scaled or unscaled data look better?
Do the same for the regions and the K = 9 clustering.*

```{r olive3, message=FALSE, warning=FALSE}
library(pdfCluster)
data("oliveoil")
str(oliveoil)
olivex=oliveoil[,3:10]
solive=scale(olivex)

# UNSCALED K=3
set.seed(1234)
olive3=kmeans(olivex, 3, nstart = 100, iter.max = 100)
t1=table(olive3$cluster,oliveoil$macro.area)

# SCALED K=3
set.seed(1234)
solive3=kmeans(solive, 3, nstart = 100, iter.max = 100)
t2=table(solive3$cluster,oliveoil$macro.area)
```

```{r tablepair, results = 'asis', echo=FALSE}
library(knitr)
library(kableExtra)
t1 %>%
  kable("html", caption = 'Unscaled Data', row.names = T) %>%
    kable_styling(full_width = F, position = "float_left")
 
t2 %>%
  kable("html", caption = 'Scaled Data', row.names = T) %>%
    kable_styling(full_width = F, position = "center")
```
ㅤ
From the analysis of the second table (concerning scaled data) we can say that yes, we have a pretty good clustering result based on the correspondence with macro-areas. First cluster represents mainly Sardinia and the South, the second one collects oils exclusively produced in the South while the third one represents the Centre-North area. However, we cannot tell the same for the results obtained from unscaled data: clusters in the first table are much more heterogeneous in terms of macro-area and so we can conclude that clustering scaled data is preferable here, and more in general, whenever measurements are not of comparable size.


ㅤ
ㅤ
```{r olive9, message=FALSE, warning=FALSE}
# UNSCALED K=9
set.seed(1234)
olive9=kmeans(olivex, 9, nstart = 100, iter.max = 100)
table(olive9$cluster,oliveoil$region) |>
kable('simple', row.names = T)

# SCALED K=9
set.seed(1234)
solive9=kmeans(solive, 9, nstart = 100, iter.max = 100)
table(solive9$cluster,oliveoil$region) |>
kable('simple', row.names = T)
```

Apparently, the same conclusions can be drawn when matching clusters to regions: results from the analysis on scaled data are much more prone to interpretation and homogeneous. As an example, the eighth and ninth clusters represent South Apulia, the first one Liguria and so on...

***
ㅤ


## **Exercise 3**
> *On Moodle you can find the data set "Boston.dat". This data set contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. Every observation refers to a "census tract", i.e. a town or district.
Visualise the data, produce a clustering of this data set that looks reasonable to you, and explain the reasons why you have chosen this and you think it is reasonable.
You can use other clustering methods that you know other than K-means, but if you use K-means only, that's fine, too.*

```{r visualize, message=FALSE, warning=FALSE, fig.align='center'}
boston<-read.table("Boston.dat",header = T,sep="")
str(boston)
boston2=boston[,-4]
sboston<-scale(boston2)

pca.boston=princomp(sboston)
biplot(pca.boston, cex=.5, main='Biplot for Standardised Boston PCA')

```

For exploratory purpose, PCA was performed on the scaled data. Most of the variance (72%) is explained by the first three components. From this Biplot, it is very hard to guess the appropriate number of clusters. We have to look more in depth.
```{r pairs, fig.align='center'}
pca.scores=pca.boston$scores[,1:4]
# Plot Matrix on principal components
library(fpc)
pairs(pca.scores, cex=1.2, col=(boston$chas+8), pch=(boston$chas+1),
      main='Plot Matrix on PCA Scores (pch = chas)')
# Levels of the rad variable
# Index of Accessibility to Radial Highways
unique(boston$rad)
pairs(pca.scores, cex=1, col=(boston$rad), pch=clusym[boston$rad],
      main='Plot Matrix on PCA Scores (pch = rad)')
```

It is clear that no evident pattern emerges from a dimensionally reduced data sample, even after taking into account both *rad* and *chas* discrete variables. Observations with max. index of accessibility to radial highways (*rad = 24*, **grey 'n' symbol**)  appear to be somehow more separated from the other data points. This will find confirmation in the following cluster analysis. 


ㅤ
```{r choice, fig.align='center', fig.height=3, message=FALSE, warning=FALSE}
library(factoextra)
set.seed(1234)
fviz_nbclust(sboston, kmeans, method = "wss", k.max = 15)
fviz_nbclust(sboston, kmeans, method = "silhouette", k.max = 15)
```
ㅤ
```{r clusgap, echo=FALSE, fig.align='center', fig.height=3.5, message=FALSE, warning=FALSE}
require(cluster)

gapnc <- function(data,FUNcluster=kmeans,
                  K.max=10, B = 100, d.power = 2,
                  spaceH0 ="scaledPCA",
                  method ="globalSEmax", SE.factor = 2,...){
    gap1 <- clusGap(data,kmeans,K.max, B, d.power,spaceH0,...)
    nc <- maxSE(gap1$Tab[,3],gap1$Tab[,4],method, SE.factor)
    kmopt <- kmeans(data,nc,...)
    out <- list()
    out$gapout <- gap1
    out$nc <- nc
    out$kmopt <- kmopt
    out
}
set.seed(1234)
prova=gapnc(sboston,K.max = 10,B=100, iter.max=50,nstart=90)
plot(prova$gapout,main = 'ClusGap Plot')

```

In order to find a reasonable number *k* of clusters, I analyzed two scree plots based on both the Total Within-Sum-of-Squares and average silhouette width (which measures the quality of a clustering solution, i.e. how well each observation lies within its cluster). A high average silhouette width indicates a good clustering.
In the first plot (TWSS) the cutoff point it's not very evident but I decided to opt for *k=5* since after that point the TWSS decreases linearly with respect to the number of cluster; this choice is also suggested by the silhouette width analysis, according to which *k=5* is the second best k-means solution. The gap statistic method doesn’t let us make a clear interpretation about the optimal k. The “globalSEmax” criterion indicates that the number of clusters should be ten, but simply they are suggesting the solution with most clusters. I would settle with a *k=5* or *6* in order to avoid groups with a reduced size.



```{r kmeans}
set.seed(1234)
library(kableExtra)
ksboston=kmeans(sboston,centers = 5,nstart = 100)
sboston_clu5=ksboston$cluster
kable(table(sboston_clu5),"simple", caption = 'Clusters Size')

```
```{r centers}
centers=matrix(NA,5,13)
for (i in 1:5) {
centers[i,]=as.vector(colMeans(boston2[sboston_clu5==i,]))  
}
centers=data.frame(centers, row.names = 1:5)
colnames(centers)=colnames(boston2)
kable(centers,'simple', row.names = T, digits = 3,
      caption="Centers in the Unscaled Feature Space")

```

```{r lastpair, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
pairs(pca.scores, cex=1, col=(sboston_clu5), pch=clusym[sboston_clu5], main='Plot Matrix on PCA Scores (pch = clusters)')
```

```{r radtable, echo=FALSE, message=FALSE, warning=FALSE}
table(ksboston$cluster,boston$rad) |>
  kable('simple',row.names = T,caption = 'Clusters Matched with Index of Accessibility to Radial Highways')
set.seed(1234)

```

We can provide a little interpretation to both clusters 1 and 3: they mainly represent "census tracts" with *rad = 24* and, from the comparison of centers, they look pretty dissimilar (*degraded and more industrialized*) from the remaining groups of tracts. The representation of the clusters in terms of standardized PCA scores is not so convincing: visual clusters are not spherical and clusters 2, 4, 5 are not well separated.

***
ㅤ


## **Exercise 4**
> *"kmeans++" is the name of a method to initialise the k-means algorithm that has been proposed in the literature. Do some research on the internet, find out and explain how this works. Run this on one or more data sets on which you have also run k-means, and compare the achieved values of the objective function.*

"kmeans++" is a seeding method proposed in 2007 by David Arthur and Sergei Vassilvitskii, which introduces a smarter starting assignment based on "spreading out" the initial k centers for the k-means algorithm. It can be seen as an enhancement of the basic random initialization which sometimes leads to poor approximate solutions in terms of objective function compared to the optimal clustering. 
Here's how it works:

1. Randomly select the first centroid from the data points
2. For each data point compute its distance from the nearest, previously chosen centroid
3. Select one new data point as next centroid such that the probability of being chosen is directly proportional to its distance from the nearest identified center (i.e. the point having maximum distance from the nearest centroid is most likely to be picked next)
4. Repeat Steps 2 and 3 until *k* centers have been sampled

The main idea of the algorithm is to initialize *the k-means* such that they are as far as possible from each other so that we increase the chances that they actually lie in different clusters. "k-means++" takes extra time at the beginning of the procedure but it also makes convergence faster.

```{r kmpp, include=FALSE}
library(pracma)
kmpp <- function(X, k) {
  n <- nrow(X)
  C <- numeric(k)
  C[1] <- sample(1:n, 1)
  for (i in 2:k) {
    dm <- distmat(X, X[C, ])
    pr <- apply(dm, 1, min); pr[C] <- 0
    C[i] <- sample(1:n, 1, prob = pr)
  }
  kmeans(X, X[C, ])
}
```

```{r comparison}
set.seed(1234)
sboston5=kmeans(sboston, 5, nstart = 1, iter.max = 100)

set.seed(1234)
kmpp.sboston5=kmpp(sboston,5)
(c(sboston5$tot.withinss,kmpp.sboston5$tot.withinss))
```

After setting "*nstart=1*" to obtain clustering results that do not take into account the randomness of initialization, we can see that the Total Within-Cluster Sum of Squares is greater in the case of the k-means classic algorithm.





