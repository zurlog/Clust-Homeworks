---
title: "Exercises 4-4"
author: "Giovanni Zurlo"
date: "8/11/2021"
output: html_document
---

## **Exercise 1**
> *Analyse the Tetragonula bees data set in R-package "prabclus". This
data set gives genetic data for 236 Tetragonula bees from Australia and
Southeast Asia. The interest here is in clustering the bees in order to find different bee species. A species is characterised by having a similar genetic makeup, whereas different species should be separated. **a)** Using tai$distmat, compute an MDS and show the MDS plot. Try out different dissimilarity-based cluster analysis methods and decide which one you think is best here. Also choose a number of clusters and visualise your final clustering using the MDS. Give reasons for your choices.
**b)** Take the points generated by the MDS and apply k-means clustering or Ward's method to them. Again choose a number of clusters.
What are advantages and disadvantages of this approach compared to the
hierarchical clustering? Do you think that this clustering is ultimately better?
Do you think it would be better, for this task, to produce an MDS solution
with p > 2?*

```{r intro, message=FALSE, warning=FALSE, cache=TRUE}
library(prabclus)
data(tetragonula)
# Loading the Tetragonula dataset
# In this form, the alleles are coded by numbers (every locus has a six digit number)

ta <- alleleconvert(strmatrix=tetragonula)
# converts the allele codes to letters, so that each locus now has two letters

tai <- alleleinit(allelematrix=ta)
# produces a collection of ways to represent the data

# Extracting the matrix of genetic distances
dist=as.dist(tai$distmat)
```

```{r mdsplot, echo=FALSE, message=FALSE, warning=FALSE,fig.align='center',cache=TRUE}
library(factoextra)
library(cluster)
library(prabclus)
library(smacof)

# Performing a MDS on Genetic DistMat to obtain 
mds=mds(dist, type = "ratio")
plot(mds$conf, main="MDS Plot for Tetragonula Genes DistMat",
     cex=1.2,pch=1,lwd=2, col='grey',
      xlim=c(-1,0.8),ylim=c(-0.8,1),
     xlab="Stress  =  26.2%",ylab='')
```

##### **Complete Linkage**
```{r completedendro, fig.align='center', fig.height=4, fig.width=10, message=FALSE, warning=FALSE,cache=TRUE}
library(ggplot2)
library(ggdendro)

# Performing a Complete Linkage AHC on Genetic Distances
complete <- hclust(dist,method="complete")
# Plotting Dendrogram
a=ggdendrogram(complete, rotate = F, theme_dendro = T,labels = F)
print(a + ggtitle("Complete Linkage Dendrogram - Genetic Distances") + xlab("") + ylab("Height"))
```

```{r , echo=FALSE, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE,cache=TRUE}
library(cluster)
tasw <- NA
tclusk <- list()
tsil <- list()
for (k in 2:30){
  tclusk[[k]] <- cutree(complete,k)
  tsil[[k]] <- silhouette(tclusk[[k]],dist=dist)
  tasw[k] <- summary(silhouette(tclusk[[k]],dist=dist))$avg.width
}

plot(1:30,tasw,type="b",ylab="ASW",
     xlab=paste("Number of clusters   -   Max ASW =",as.character(round(max(tasw[-1]),3))))
k.complete=match(max(tasw[-1]),tasw[-1])+1
abline(v=k.complete,col="red",lty="dashed")

```

According to the Average Silhouette Width criteria, the suggested number of clusters is **12**.

ㅤ

##### **Average Linkage**

```{r avgdendro, fig.align='center', fig.height=4, fig.width=10, message=FALSE, warning=FALSE,cache=TRUE}
# Performing an Average Linkage AHC on Genetic Distances
average <- hclust(dist,method="average")
b=ggdendrogram(average, rotate = F, theme_dendro = T,labels = F)
print(b + ggtitle("Average Linkage Dendrogram - Genetic Distances") + xlab("") + ylab("Height"))
```

```{r echo=FALSE, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE,cache=T}
library(cluster)

tasw <- NA
tclusk <- list()
tsil <- list()
for (k in 2:30){
  tclusk[[k]] <- cutree(average,k)
  tsil[[k]] <- silhouette(tclusk[[k]],dist=dist)
  tasw[k] <- summary(silhouette(tclusk[[k]],dist=dist))$avg.width
}
plot(1:30,tasw,type="b",ylab="ASW",
     xlab=paste("Number of clusters   -   Max ASW =",as.character(round(max(tasw[-1]),3))))
k.average=match(max(tasw[-1]),tasw[-1])+1
abline(v=k.average,col="red",lty="dashed")

```

According to the ASW criteria, the suggested number of clusters is **10**.

ㅤ

##### **PAM Method**

```{r pam, echo=TRUE, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE, cache=T}
pasw <- NA
pclusk <- list()
psil <- list()
for (k in 2:25){
  # PAM on Genetic Distances
  pclusk[[k]] <- pam(dist,k)
  # Computation of Silhouettes:
  psil[[k]] <- silhouette(pclusk[[k]],dist=dist)
  # ASW index retrieval:
  pasw[k] <- summary(psil[[k]])$avg.width}
```
```{r plot, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE, cache=T}
# Plotting the ASW graph and showing the suggested N. of clusters
plot(1:25,pasw,type="b",ylab="ASW",
xlab=paste("Number of clusters   -   Max ASW =",as.character(round(max(pasw[-1]),3))))
m=match(max(pasw[-1]),pasw[-1])+1
abline(v=m,col="red",lty="dashed")
```


According to the ASW criteria, the suggested number of clusters is **10**.
The highest value of the ASW is provided by the average linkage clustering solution, so I opted for this partition which can be considered satisfactory. MDS plot should be interpreted with care, since it do not represent directly the actual goodness of clustering we assess from the Silhouette Plot.
```{r tables, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
summary<- matrix(0, nrow = 1, ncol = 3)

colnames(summary)=c("Complete","Average","Pam")
rownames(summary)=c("ASW")
summary[1,]=c(0.472,0.486,0.468)

library(kableExtra)
summary |>
  kable("html", caption = '', row.names = T) |>
    kable_styling(full_width = F, position = "center")
```

```{r plott, echo=FALSE, fig.align='center', fig.width=8, message=FALSE, warning=FALSE, cache=TRUE}
average.out = cutree(average,k.average)
average.f=as.factor(average.out)

mds.df=as.data.frame(mds$conf)
mds.df= cbind(mds.df,average.f)

library(fpc)
library(ggplot2)
library(ggdendro)
library(ggpubr)

gg1=ggscatter(mds.df,
               x="D1", y="D2",
               color = "average.f", palette = "simpsons",
               shape = "average.f",
               ellipse = F, 
               mean.point = TRUE,
               star.plot = TRUE)
print(gg1 + ggtitle("MDS Plot for the Genetic DistMat - Average Linkage (10)")+theme(legend.position = "none"))
```
```{r silplot1, echo=FALSE, fig.align='center', fig.height=7, fig.width=7, message=FALSE, warning=FALSE, cache=TRUE}
plot(tsil[[10]], main="Silhouette Plot for the Average Linkage Solution (10)")
```

ㅤ

##### **K-Means on MDS Fitted Configurations**
Now we will follow an alternative way of clustering the data by working on the MDS fitted configurations that we already used to approximates dissimilarities in a euclidean space.
```{r clusgap, message=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.height=4,fig.width=8}
mds=mds(dist, type = "ratio")
set.seed(1234)
# Computing the Gap Statistic for each k-means solution up to k = 14
cg1 <- clusGap(mds$conf,kmeans,14,B=200,d.power=2,spaceH0="scaledPCA",nstart=200)
plot(cg1,main="ClusGap Plot for K-means",xlab="Number of Clusters",ylab="Gap Statistic")
abline(v=8,col="red",lty="dashed")
```

```{r silh.criteria, echo=FALSE, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE,cache=T}
tasw <- NA
tclusk <- list()
tsil <- list()
for (k in 2:20){
  tclusk[[k]] <- kmeans(mds$conf,centers=k,iter.max = 100,nstart = 100)
  tsil[[k]] <- silhouette(tclusk[[k]]$cluster,dist=dist(mds$conf))
  tasw[k] <- summary(tsil[[k]])$avg.width
}
plot(1:20,tasw,type="b",ylab="ASW",
     xlab=paste("Number of clusters   -   Max ASW =",as.character(round(max(tasw[-1]),3))))
grid()
m=match(max(tasw[-1]),tasw[-1])+1
abline(v=m,col="red",lty="dashed")

```

The choice here is between *K = 7* and *K = 8*; I opted for the first one since we have a significant drop in the ASW if we look for one more cluster usign k-means.

```{r kmeans7, echo=FALSE, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(1234)
kmeans.out=kmeans(mds$conf,7,iter.max = 300,nstart = 300)
kmeans.f=as.factor(kmeans.out$cluster)

mds.df=as.data.frame(mds$conf)
mds.df= cbind(mds.df,kmeans.f)

gg1=ggscatter(mds.df,
              x="D1", y="D2",
              color = "kmeans.f", palette = "simpsons",
              shape = "kmeans.f",
              ellipse = F, 
              mean.point = TRUE,
              star.plot = TRUE)
gg1+theme(legend.position = "none")+ggtitle("KMeans (7) on MDS Fitted Configurations")

```

ㅤ


Differently from the clusterings based on the genetic distances matrix, this partition "looks good" when it is reported on the MDS plot. That's because the algorithm is specifically applied to the fitted configuration we obtained, which itself shows some homogeneous and separated clusters of points.

**All Perfect? Not Really**
```{r km_real, fig.align='center', fig.width=8, message=FALSE, warning=FALSE, cache=TRUE}
library(mclust)
library(cluster)
# Performing a comparison between K-Means (7) and the previous partition
adjustedRandIndex(kmeans.f,average.f)

# Assessing the clustering quality of K-Means (7) wrt the actual genetic DistMat
real_ksil<- silhouette(kmeans.out$cluster,as.dist(tai$distmat))
plot(real_ksil,main="Silhouette Plot for the K-Means Clustering on MDS Config.")
```

ㅤ



Such K-Means partition is based on a euclidean approximated representation of the original genetic distances matrix, which implies a significant loss of information (recall that stress was around 26%). It is not much different from the previous average linkage partition (*ARI = 0.94*) since several clusters maintain their structure even when "compressed" in a 2D space. However, when the k-means (7) partition is evaluated with respect to the original distance matrix, it reveals a lower goodness of clustering (*ASW = 0.42*). However, one advantage of such approach is that it can be computationally more efficient (especially in the case of k-means for bivariate data) and can lead to more appealing and interpretable visualizations of the results. It can be a reasonable strategy whenever the stress index is low and/or we can afford such loss of information and goodness of the partition.

##### **K-Means on MDS 3D Fitted Configurations**
```{r km11_real, fig.align='center', fig.height=7, fig.width=8, message=FALSE, warning=FALSE, cache=TRUE}
# Performing MDS on a 3D euclidean space
mds3=mds(dist,ndim=3,type = "ratio")
# Reduction in the stress, quite a bit of information not represented yet
mds3$stress
set.seed(1234)
# Applying K-Means on the 3D Fitted Configuration after choosing  
kmeans11.out=kmeans(mds3$conf,11,iter.max = 300,nstart = 300)
kmeans11.f=as.factor(kmeans11.out$cluster)

mds.df=as.data.frame(mds3$conf)
mds.df= cbind(mds.df,kmeans11.f)

adjustedRandIndex(kmeans11.f,average.f)

real_k11sil<- silhouette(kmeans11.out$cluster,as.dist(tai$distmat))
plot(real_k11sil,main="Silhouette Plot for the K-Means Clustering on 3D MDS Config.")
```

ㅤ




Things slightly improve if we perform the analysis on a 3D MDS configuration (*ASW = 0.44*), since stress reduces significantly. The partition we obtain is, however, less similar to the average linkage one and we lose the interpretation and visualization advantages of the previous solution (even if we could resort to 3D plotting).

***
ㅤ

## **Exercise 2**
> *For the olive oil data with standardised variables, compute the following
clusterings and compare them according to their similarity (ARI) with both the
macro areas and the regions.*

ㅤ





##### **K-Means with number of clusters estimated by the gap statistic**

```{r kmeans18, message=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.height=4,fig.width=8}
library(pdfCluster)
library(cluster)
library(mclust)
# Loading and pre-processing data
data("oliveoil")
olivex=oliveoil[,3:10]
solive=scale(olivex)

set.seed(1234)
# Computing the Gap Statistic for each k-means solution up to k = 20
cg1 <- clusGap(solive,kmeans,20,B=100,d.power=2,spaceH0="scaledPCA",nstart=100)
plot(cg1,main="ClusGap Plot for K-means",xlab="Number of Clusters",ylab="Gap Statistic")
abline(v=18,col="red",lty="dashed")
```

```{r kmeans18_table, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
library(kableExtra)
rownomi=c("Macro Area","Region")

a=cbind(0.19,0.424) 
colnames(a)=rownomi  
rownames(a)="K-Means (18)"
a |>
  kable("html", caption = 'Clustering Similarity measured by ARI', row.names = T) |>
  kable_styling(full_width = F, position = "center")
```

ㅤ





##### **Ward with number of clusters estimated by the gap statistic**
```{r ward16, message=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.height=3, fig.width=7}
library(factoextra)
set.seed(1234)
# Computing the Gap Statistic for each Ward partition up to k = 20
fviz_nbclust(solive, FUNcluster = hcut, k.max = 20, hc_method = "ward.D2",
 method = "gap_stat", maxSE = list(method = "globalSEmax", SE.factor = 2))
```

```{r ward16_table, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
a=cbind(0.209,0.472) 
colnames(a)=rownomi  
rownames(a)="Ward.D2 (16)"
a |>
  kable("html", caption = 'Clustering Similarity measured by ARI', row.names = T) |>
  kable_styling(full_width = F, position = "center")
```

ㅤ





##### **Single Linkage using Euclidean, Manhattan, and Mahalanobis distance**

```{r single.eucl2, message=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.height=3, fig.width=7}
fviz_nbclust(solive, FUNcluster = hcut, k.max = 20, hc_method = "single",
             hc_metric="euclidean", method = "silhouette")

single.eucl2=hclust(dist(solive),method = "single") |>
  cutree(k=2)
adj.rand.index(single.eucl2,oliveoil$macro.area)
adj.rand.index(single.eucl2,oliveoil$region)
```

In this case the ASW criterion shows its tendency to favour *K = 2*. Similarity between this partition and the covariates groupings is approx. 0 as measured by the ARI. The exploration of the local optima at *K = 5* led to remarkable ARI values, so this partition was preferred.
With Manhattan distance, the local maxima *K = 7* was chosen.

```{r mahala_eucl, echo=FALSE, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE,cache=T}
mahaladist <- matrix(0,ncol=572,nrow=572)
olivecov <- cov(olivex)
for (i in 1:572) {
  mahaladist[i,] <- mahalanobis(olivex,as.numeric(olivex[i,]),olivecov)}
mahaladist=as.dist(mahaladist)
mahalanobis <- hclust(mahaladist,method="single")

tasw <- NA
tclusk <- list()
tsil <- list()
for (k in 2:20){
  tclusk[[k]] <- cutree(mahalanobis,k)
  tsil[[k]] <- silhouette(tclusk[[k]],dist=mahaladist)
  tasw[k] <- summary(silhouette(tclusk[[k]],dist=mahaladist))$avg.width}

plot(1:20,tasw,type="b",ylab="ASW",
     xlab=paste("Number of clusters   -   Max ASW =",as.character(round(max(tasw[-1]),3))),
     main="ASW Plot - Single Linkage AHC on Mahalanobis DistMat")
k.mahala=match(max(tasw[-1]),tasw[-1])+1
abline(v=k.mahala,col="red",lty="dashed")

```

ASW analysis on single linkage AHC with Mahalanobis distance led to this monotonically decreasing curve. The *K = 5* partition has been arbitrarly chosen.

```{r singlelink, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
a=rbind(0.81,0.356)
b=rbind(0.794,0.346)
c=rbind(0.008,0.003)
table=cbind(a,b,c)
colnomi=c('Euclidean (5)','Manhattan (7)','Mahalanobis (5)')
colnames(table)=colnomi  
rownames(table)=rownomi
table |>
  kable("html", caption = 'ARI - Single Linkage AHC', row.names = T) |>
  kable_styling(full_width = F, position = "center")

```

ㅤ





##### **Average Linkage using Euclidean, Manhattan, and Mahalanobis distance**

```{r mahala_avg, echo=FALSE, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE,cache=T}
library(cluster)
mahaladist <- matrix(0,ncol=572,nrow=572)
olivecov <- cov(olivex)
for (i in 1:572) {
  mahaladist[i,] <- mahalanobis(olivex,as.numeric(olivex[i,]),olivecov)
}
mahaladist=as.dist(mahaladist)
mahalanobis <- hclust(mahaladist,method="average")

tasw <- NA
tclusk <- list()
tsil <- list()
for (k in 2:20){
  tclusk[[k]] <- cutree(mahalanobis,k)
  tsil[[k]] <- silhouette(tclusk[[k]],dist=mahaladist)
  tasw[k] <- summary(silhouette(tclusk[[k]],dist=mahaladist))$avg.width
}

plot(1:20,tasw,type="b",ylab="ASW",
     xlab=paste("Number of clusters   -   Max ASW =",as.character(round(max(tasw[-1]),3))),
     main="ASW Plot - Average Linkage AHC on Mahalanobis DistMat")
abline(v=12,col="red",lty="dashed")
```

```{r avglink, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
a=rbind(0.559,0.827)
b=rbind(0.481,0.777)
c=rbind(0.124,0.171)
table=cbind(a,b,c)
colnomi=c('Euclidean (10)','Manhattan (8)','Mahalanobis (12)')
colnames(table)=colnomi  
rownames(table)=rownomi
table |>
  kable("html", caption = 'ARI - Average Linkage AHC', row.names = T) |>
  kable_styling(full_width = F, position = "center")
```

ㅤ





##### **Complete Linkage using Euclidean, Manhattan, and Mahalanobis distance**

```{r complete.eucl2, message=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.height=3, fig.width=7}
fviz_nbclust(solive, FUNcluster = hcut, k.max = 20, hc_method = "complete",
             hc_metric="euclidean", method = "silhouette")
```

The ASW criterion shows its tendency to favour *K = 2*. Similarity between this partition and the covariates groupings is approximately 0 as measured by the ARI. The exploration of the local optima at *K = 10* led to significant ARI values, so this partition was preferred.
With Manhattan distances, the global maxima *K = 7* was chosen.

```{r complink, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
a=rbind(0.431,0.670)
b=rbind(0.458,0.673)
c=rbind(0.274,0.520)
table=cbind(a,b,c)
colnomi=c('Euclidean (10)','Manhattan (7)','Mahalanobis (20)')
colnames(table)=colnomi  
rownames(table)=rownomi
table |>
  kable("html", caption = 'ARI - Complete Linkage AHC', row.names = T) |>
  kable_styling(full_width = F, position = "center")
```

ㅤ





##### **PAM using Euclidean, Manhattan, and Mahalanobis distance**
```{r pam.eucl5, echo=FALSE, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE,cache=T}
pasw <- NA
pclusk <- list()
psil <- list()
set.seed(1234)
for (k in 2:20){
  pclusk[[k]] <- pam(solive,k, metric = 'euclidean', keep.diss = T)
  # Computation of silhouettes:
  psil[[k]] <- silhouette(pclusk[[k]])
  # ASW needs to be extracted:
  pasw[k] <- summary(psil[[k]])$avg.width
}


plot(1:20,pasw,type="b",ylab="ASW",
     xlab=paste("Number of clusters   -   Max ASW =",as.character(round(max(pasw[-1]),3))),
     main="ASW Plot - PAM Algorithm on Euclidean DistMat")
m=match(max(pasw[-1]),pasw[-1])+1
abline(v=m,col="red",lty="dashed")
```

Global optimas in the ASW plots led to the choice of *K = 5* in the case of euclidean and manhattan distances, while the usage of the Mahalanobis distance showed the optimality of *K = 6*. Here clustering similarity measures are comparable across the adopted metrics.  

```{r pamtable, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
a=rbind(0.559,0.755)
b=rbind(0.553,0.757)
c=rbind(0.597,0.712)
table=cbind(a,b,c)
colnomi=c('Euclidean (5)','Manhattan (5)','Mahalanobis (6)')
colnames(table)=colnomi
rownomi=c("Macro Area","Region")
rownames(table)=rownomi
table |>
  kable("html", caption = 'ARI - PAM Algorithm', row.names = T) |>
  kable_styling(full_width = F, position = "center")
```

***
ㅤ

## **Exercise 4**
> *Read the DBSCAN paper and answer the following questions: **(a)** Summarize in your own words what the DBSCAN method does. **(b)** What are the advantages, according to the authors, of their DBSCAN method compared with other clustering methods, particularly those that you already know? Do you think that the authors' arguments are convincing? **(c)** Find out how to run DBSCAN in R and apply it to the Bundestag data.*

### **a)**
DBSCAN looks for clusters conformable to a specific definition:

- In order to belong to a certain cluster *C*, a point must be density-reachable from at least one point that belongs to that cluster. [**Maximality**]

- All the points belonging to a cluster *C* must be *at least* density connected [**Connectivity**]

So each point in a domain *D* either belongs to a cluster or it is classified as "noise". Noise will be defined relative to a given set of clusters. The DBSCAN has two parameters (requiring, however, only one of them):

- *MinPts*: the minumum number of points that are directly density-reachable from a core point p, that is, roughly speaking, a point inside the cluster.

- *Eps*: is a threshold distance between two points *p* and *q* such that *q* can be considered to belong to the Eps-Neighborhood of p.

The algorithm starts by visiting an arbitrary point *p* and retriving its Eps-neighborhood: if the number of points belonging to it are larger than *MinPts* (so it satisfies the core point condition), a new cluster is started, otherwise the point will be labeled as noise. The process of retrieving the Eps-neighborhood is done for every point belonging to the original one until all density-connected points from p are found, and this will define the first cluster. This is done for all the remaining unvisited points and this will result in finding a new cluster or labeling a point as noise.
Merging of two contiguous clusters into one is avoided if they are separated from each other in a way that the smallest distance between two of their points is larger then Eps.



### **b)**

According to the authors we have three main advantages when applying DBSCAN to large databases:

1) *Minimal requirements of domain knowledge to determine the input parameters*

We have to provide just one parameter (*Eps*) to make the algorithm work and its choice is somehow supported. This is especially important when dealing with large data since we often do not have prior knowledge about parameters.
As a comparison, for partitioing algorithms like kmeans we have to provide the number of clusters *k* and most of the times we have to go through many possible choices in order to evaluate and pick one of them. Also for hierarchical algorithms we have problems identifying the parameters for a termination condition that properly separetes the clusters.

2) *Discovery of clusters with arbitrary shape*

Search is not subject to restrictive conditions on the shape: as an example, all clusters found by partitioning algorithms are convex (or spherical), while for hierarchical ones this is related to the type of linkage adopted.

3) *Good efficiency on large databases*

The above mentioned problems can be solved for partitioning and hierarchical algorithms by some upgraded versions of the classical ones like CLARANS (based on PAM) and Ejcluster (divisive hierarchical algo), but DBSCAN offers a less computationally demanding solution. In the paper it is shown that DBSCAN outperforms CLARANS by a factor of between 250 and 1900, and this grows with increasing size of the database.

The authors' arguments seem very convincing to me since they tackle and overcome real data analysis issues in a specific field of application (large spatial databases with noise). DBSCAN seems very promising in some kind of problems but may not be a good choice in some other. In some sense the authors wanted to fill a void in the scientific literature by designing a first algorithm with all the three characteristics mentioned before.


### **c)**

```{r distplot, fig.align='center', fig.height=4, message=FALSE, warning=FALSE, cache=TRUE}
# Loading data
bundestag=read.table(file.choose(),header = T,sep="")
bundestag$ewb=as.factor(bundestag$ewb)
bundestag$state=as.factor(bundestag$state)
data=bundestag[,1:5]

# Sorted k-dist graph
kNNdistplot(data, k=6)
abline(h=0.068, col=2, lty=2)
```
In order to make DBSCAN work we have to decide the value of *Eps* and *MinPts*. Usually a rule of thumb is to set MinPts = 4 for 2-dimensional data, and it should be chosen proportionally to the size of the dataset, so in our case MinPts=6. DBSCAN is sensitive to the choice of *Eps*, in particular if clusters have different densities. If *Eps* is too small, sparser clusters will be defined as noise. If *Eps* is too large, denser clusters may be merged together. One tool to define Eps is the kNNdistplot, where the distances from each point to its k-th nearest neighbor are sorted in ascending order. The idea is that points inside a cluster will a have small value of the k-nearest neighbor distance because they are close to other points in the same cluster, while noise points are isolated and will have a rather large kNN distance. So we have to find an elbow on the curve and points after that threshold will be regarded as noise. In this case we consider Eps=0.068.
```{r dbscanb, message=FALSE, warning=FALSE, fig.align='center', cache=TRUE}
library(dbscan)
library(fpc)
# dbscan package provides a faster re-implementation of the algorithm
out.dbs=dbscan::dbscan(data, eps = 0.068, minPts = 6)
print(out.dbs)
# fpc package let us know the number of points belonging to the clusters that are seeds and border points
out.dbs2=fpc::dbscan(data, eps = 0.068, MinPts = 6)
print(out.dbs2)
```
```{r last}
# Plotting data highlighting the obtained clusters
pairs(data,cex=0.8,col=(out.dbs$cluster+5))
# Plotting data highlighting the macro-areas
pairs(data,cex=0.8,col=as.numeric(bundestag$ewb)+3)
# Comparing the obtained clustering with the macro-areas
mclust::adjustedRandIndex(out.dbs$cluster,bundestag$ewb)
```
As we can see the clusters produced by DBSCAN seem to match pretty well the macro-areas both from the graphical point of view and ARI, with noise data-points collecting some voting districts in the Berlin area (*cyan in the first plot, blue in the second one*).

***
ㅤ












