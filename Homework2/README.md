## **(1)**
* Use the gap statistic method to estimate the number of clusters for the olive oil data, and for Artificial Dataset2. Comment on the solutions.
* Generate a dataset from a two-dimensional uniform distribution on the rectangle [min x1; max x1] x [min x2; max x2], where x1; x2 are the values of the first and second variable of Artificial Dataset2. Compute K-means clusterings for K from 1 to 10 and show at least three scatterplots of the dataset with clusterings with different K. Compare the values of log Sk from clustering Artificial Dataset2 with those from clustering the uniformly distributed dataset


## **(2)**
The R-function clusGap offers various options, for example `spaceH0="original"` (using
a rectangle along the main axes for the uniform distribution) and
`SE.factor=1` (q = 1 for the factor q with which the standard error of Gap(K) is multiplied,
as suggested in Tibshirani et al.'s original paper). We may be interested in whether these
options could improve the results compared to the options `spaceH0="scaledPCA"` and
`SE.factor=2` as used in the lecture.
As every spaceH0 choice can be combined with every SE.factor choice, these options
define four different ways to run the clusGap function.
The course notes give the code to generate data sets with the same distribution as the
Artificial Data Set 1. Different data sets of the same kind can be generated by not fixing
`set.seed` before generating every individual data set (it may be fixed before generating
any data for making the simulation results reproducible).
Using the gapnc-function that automatically estimates the number of clusters for a data
set with clusGap with given choices for spaceH0 and SE.factor, generate 100 data sets
according to the specifications for Artificial Data Set 1, estimate the number of clusters
by all four possible ways to run clusGap and compile four different vectors with all 100
estimated numbers of clusters for each version of clusGap. At the end look at the four dis-
tributions of estimated numbers of clusters using a suitable graphical display and comment
on them. Are there better or worse results for some of the clusGap-versions?


## **(3)**
Show that Simple Matching and Mahalanobis distance are dissimilarities and also distances (i.e., fulfill the triangle inequality).

## **(5)**

* A political scientist wants to cluster the respondents of a questionnaire using a distance-
based method. The questionnaire has preference questions with two response options
of the type "do you prefer the current level of taxes, or do you prefer higher taxes
with all money from the higher taxes being invested in the health system?" Generally
the option "I prefer the current situation" is coded 0 and the option that suggests
something else is coded 1. Would you prefer the simple matching distance or the
Jaccard distance here? Why?
* Geographers want to cluster areas in the Swiss alps according to danger from avalanches
in order to produce a map with dierent colour codes for different danger levels, using
a distance-based method. Their variables are, all for the year 2019: (i) the number of
avalanches in the area, (ii) the average percentage of the area covered by an avalanche,
(iii) number of persons injured or dead in incidents involving avalanches in the area,
(iv) Swiss Francs investment in the security of the ski slopes in the area, (v) Swiss
Francs budget for emergency rescue in the area. Would you prefer the Euclidean
distance on raw data, the Euclidean distance on scaled data, the Manhattan distance
on raw data, the Manhattan distance on scaled data, or the Mahalanobis distance for
these data? Why? (Probably more than one option can convincingly be argued.)
